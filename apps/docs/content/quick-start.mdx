---
title: Quickstart
description: Fastest way to start using LLMÂ Gateway in any language or framework.
---

import { Accordion, Accordions } from "fumadocs-ui/components/accordion";
import { Tabs, Tab } from "fumadocs-ui/components/tabs";
import { DynamicCodeBlock } from "fumadocs-ui/components/dynamic-codeblock";

# ðŸš€ Quickstart

Welcome to **LLMÂ Gateway**â€”a single dropâ€‘in endpoint that lets you call todayâ€™s best largeâ€‘language models while keeping **your existing code** and development workflow intact.

> **TL;DR**Â â€” Point your HTTP requests to `https://api.llmgateway.io/v1/â€¦`, supply your `LLM_GATEWAY_API_KEY`, and youâ€™re done.

---

## 1Â Â·Â Get an API key

1. Sign in to the dashboard.
2. Create a new Project â†’ _Copy the key_.
3. Export it in your shell (or a `.env` file):

```bash
export LLM_GATEWAY_API_KEY="llmgtwy_XXXXXXXXXXXXXXXX"
```

---

## 2 Â· Pick your language

<Tabs groupId="language" items={[
  'cURL',
  'TypeScript',
  'React',
  'Next.js',
  'Python',
  'Java',
  'Rust',
  'Go',
  'PHP',
  'Ruby']}
  persist
>
  <Tab value="cURL">
    <DynamicCodeBlock lang="bash" code={`curl -X POST https://api.llmgateway.io/v1/chat/completions \\
  -H "Content-Type: application/json" \\
  -H "Authorization: Bearer $LLM_GATEWAY_API_KEY" \\
  -d '{
  "model": "gpt-4o",
  "messages": [
    {"role": "user", "content": "Hello, how are you?"}
  ]
}'`} />
  </Tab>
  <Tab value="TypeScript">
    <DynamicCodeBlock lang="typescript" code={`const response = await fetch('https://api.llmgateway.io/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': \`Bearer \${process.env.LLM_GATEWAY_API_KEY}\`
  },
  body: JSON.stringify({
    model: 'gpt-4o',
    messages: [
      { role: 'user', content: 'Hello, how are you?' }
    ]
  })
});

if (!response.ok) {
throw new Error(\`HTTP error! status: \${response.status}\`);
}

const data = await response.json();
console.log(data.choices[0].message.content);`} />

  </Tab>
  <Tab value="React">
    <DynamicCodeBlock lang="tsx" code={`import { useState } from 'react'

function ChatComponent() {
const [response, setResponse] = useState('');
const [loading, setLoading] = useState(false);

const sendMessage = async () => {
setLoading(true);
try {
const res = await fetch('https://api.llmgateway.io/v1/chat/completions', {
method: 'POST',
headers: {
'Content-Type': 'application/json',
'Authorization': \`Bearer \${process.env.REACT_APP_LLM_GATEWAY_API_KEY}\`
},
body: JSON.stringify({
model: 'gpt-4o',
messages: [
{ role: 'user', content: 'Hello, how are you?' }
]
})
});

      if (!res.ok) {
        throw new Error(\`HTTP error! status: \${res.status}\`);
      }

      const data = await res.json();
      setResponse(data.choices[0].message.content);
    } catch (error) {
      console.error('Error:', error);
    } finally {
      setLoading(false);
    }

};

return (

<div>
	<button onClick={sendMessage} disabled={loading}>
		{loading ? "Sending..." : "Send Message"}
	</button>
	{response && <p>{response}</p>}
</div>
); }

export default ChatComponent;
`} />
  </Tab>
  <Tab value="Next.js">
    <DynamicCodeBlock lang="typescript" code={`; // app/api/chat/route.ts
import { NextRequest, NextResponse } from "next/server";

export async function POST(request: NextRequest) {
  const { message } = await request.json();

const response = await fetch('https://api.llmgateway.io/v1/chat/completions', {
method: 'POST',
headers: {
'Content-Type': 'application/json',
'Authorization': \`Bearer \${process.env.LLM_GATEWAY_API_KEY}\`
},
body: JSON.stringify({
model: 'gpt-4o',
messages: [
{ role: 'user', content: message }
]
})
});

if (!response.ok) {
return NextResponse.json({ error: 'Failed to get response' }, { status: response.status });
}

const data = await response.json();

return NextResponse.json({
message: data.choices[0].message.content
});
}

// Usage in component:
// const response = await fetch('/api/chat', {
// method: 'POST',
// headers: { 'Content-Type': 'application/json' },
// body: JSON.stringify({ message: 'Hello, how are you?' })
// });`} />

  </Tab>
  <Tab value="Python">
    <DynamicCodeBlock lang="python" code={`import requests
import os

response = requests.post(
'https://api.llmgateway.io/v1/chat/completions',
headers={
'Content-Type': 'application/json',
'Authorization': f'Bearer {os.getenv("LLM_GATEWAY_API_KEY")}'
},
json={
'model': 'gpt-4o',
'messages': [
{'role': 'user', 'content': 'Hello, how are you?'}
]
}
)

response.raise_for_status()
print(response.json()['choices'][0]['message']['content'])`} />

  </Tab>
  <Tab value="Java">
    <DynamicCodeBlock lang="java" code={`import java.net.http.HttpClient;
import java.net.http.HttpRequest;
import java.net.http.HttpResponse;
import java.net.URI;

String apiKey = System.getenv("LLM_GATEWAY_API_KEY");
String requestBody = """
{
\"model\": \"gpt-4o\",
\"messages\": [
{\"role\": \"user\", \"content\": \"Hello, how are you?\"}
]
}
""";

HttpRequest request = HttpRequest.newBuilder()
.uri(URI.create("https://api.llmgateway.io/v1/chat/completions"))
.header("Content-Type", "application/json")
.header("Authorization", "Bearer " + apiKey)
.POST(HttpRequest.BodyPublishers.ofString(requestBody))
.build();

HttpResponse<String> response = HttpClient.newHttpClient()
.send(request, HttpResponse.BodyHandlers.ofString());

System.out.println(response.body());`} />

  </Tab>
  <Tab value="Rust">
    <DynamicCodeBlock lang="rust" code={`use reqwest::Client;
use serde_json::json;
use std::env;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
let client = Client::new();
let api_key = env::var("LLM_GATEWAY_API_KEY")?;

    let response = client
        .post("https://api.llmgateway.io/v1/chat/completions")
        .header("Content-Type", "application/json")
        .header("Authorization", format!("Bearer {}", api_key))
        .json(&json!({
            "model": "gpt-4o",
            "messages": [
                {"role": "user", "content": "Hello, how are you?"}
            ]
        }))
        .send()
        .await?;

    let result: serde_json::Value = response.json().await?;
    println!("{}", result["choices"][0]["message"]["content"]);
    Ok(())

}`} />

  </Tab>
  <Tab value="Go">
    <DynamicCodeBlock lang="go" code={`package main

import (
    "bytes"
    "encoding/json"
    "fmt"
    "net/http"
    "os"
)

type ChatRequest struct {
Model string ` + "`json:\"model\"`" + `
Messages []Message ` + "`json:\"messages\"`" + `
}

type Message struct {
Role string ` + "`json:\"role\"`" + `
Content string ` + "`json:\"content\"`" + `
}

func main() {
apiKey := os.Getenv("LLM_GATEWAY_API_KEY")

    requestBody := ChatRequest{
        Model: "gpt-4o",
        Messages: []Message{{Role: "user", Content: "Hello, how are you?"}},
    }

    jsonData, _ := json.Marshal(requestBody)

    req, _ := http.NewRequest("POST", "https://api.llmgateway.io/v1/chat/completions", bytes.NewBuffer(jsonData))
    req.Header.Set("Content-Type", "application/json")
    req.Header.Set("Authorization", "Bearer "+apiKey)

    client := &http.Client{}
    resp, _ := client.Do(req)
    defer resp.Body.Close()

    fmt.Println("Response received")

}`} />

  </Tab>
  <Tab value="PHP">
    <DynamicCodeBlock lang="php" code={`<?php
$apiKey = $_ENV['LLM_GATEWAY_API_KEY'];

$data = [
'model' => 'gpt-4o',
'messages' => [
['role' => 'user', 'content' => 'Hello, how are you?']
]
];

$options = [
    'http' => [
        'header' => [
            'Content-Type: application/json',
            'Authorization: Bearer ' . $apiKey
        ],
        'method' => 'POST',
        'content' => json_encode($data)
]
];

$context = stream_context_create($options);
$response = file_get_contents(
'https://api.llmgateway.io/v1/chat/completions',
false,
$context
);

if ($response === FALSE) {
throw new Exception('Request failed');
}

$result = json_decode($response, true);
echo $result['choices'][0]['message']['content'];
?>`} />

  </Tab>
  <Tab value="Ruby">
    <DynamicCodeBlock lang="ruby" code={`require 'net/http'
require 'json'
require 'uri'

uri = URI('https://api.llmgateway.io/v1/chat/completions')
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri)
request['Content-Type'] = 'application/json'
request['Authorization'] = "Bearer #{ENV['LLM_GATEWAY_API_KEY']}"

request.body = {
model: 'gpt-4o',
messages: [
{ role: 'user', content: 'Hello, how are you?' }
]
}.to_json

response = http.request(request)

if response.code != '200'
raise "HTTP Error: #{response.code}"
end

result = JSON.parse(response.body)
puts result['choices'][0]['message']['content']`} />

  </Tab>
</Tabs>

---

## 3 Â· SDK integrations

```ts title="vercel-ai-sdk.ts"
import { createOpenAI } from "@ai-sdk/openai";

const llmgateway = createOpenAI({
	baseURL: "https://api.llmgateway.io/v1",
	apiKey: process.env.LLM_GATEWAY_API_KEY!,
});

const completion = await llmgateway.chat({
	model: "gpt-4o",
	messages: [{ role: "user", content: "Hello, how are you?" }],
});

console.log(completion.choices[0].message.content);
```

```ts title="openai-sdk.ts"
import OpenAI from "openai";

const openai = new OpenAI({
	baseURL: "https://api.llmgateway.io/v1",
	apiKey: process.env.LLM_GATEWAY_API_KEY,
});

const completion = await openai.chat.completions.create({
	model: "gpt-4o",
	messages: [{ role: "user", content: "Hello, how are you?" }],
});

console.log(completion.choices[0].message.content);
```

---

## 4Â Â·Â Going further

- **Streaming**: pass `stream: true` to any requestâ€”Gateway will proxy the event stream unchanged.
- **Monitoring**: Every call appears in the dashboard with latency, cost & provider breakdown.
- **Failâ€‘over**: Specify `fallback_models` to autoâ€‘retry on provider errors.

---

## 5Â Â·Â FAQ

<Accordions type="single">
	<Accordion title="Which models are supported?">
		Currently `gptâ€‘4o`, `gptâ€‘4â€‘turbo`, `gptâ€‘3.5â€‘turbo`, `claudeâ€‘3` and
		`geminiâ€‘1.5`. More on the [Models page](https://llmgateway.io/models).
	</Accordion>
	<Accordion title="What makes your service different from OpenRouter?">
		<p>Unlike OpenRouter, we offer:</p>
		<ul className="list-disc pl-6 mt-2 space-y-1">
			<li>
				Full self-hosting capabilities with an MIT license, giving you complete
				control over your infrastructure
			</li>
			<li>
				Enhanced analytics with deeper insights into your model usage and
				performance
			</li>
			<li>
				No fees when using your own provider keys, maximizing cost efficiency
			</li>
			<li>
				Greater flexibility and customization options for enterprise deployments
			</li>
		</ul>
	</Accordion>
	<Accordion title="How much do you charge for your services?">
		<p>Our pricing structure is designed to be flexible and cost-effective:</p>
		<ul className="list-disc pl-6 mt-2 space-y-1">
			<li>
				<strong>Free tier:</strong> No fees when providing your own provider API
				keys
			</li>
			<li>
				<strong>Self-hosted option:</strong> Fully self-host our solution for
				free with our MIT-licensed software
			</li>
			<li>
				<strong>Premium features:</strong> Access to advanced analytics and
				deeper insights
			</li>
		</ul>
		<p className="mt-2">
			We're currently finalizing our complete pricing structure for managed
			services and enterprise solutions. Contact us for more details on custom
			pricing options.
		</p>
	</Accordion>
</Accordions>

---

## 6Â Â·Â Next steps

- Read [Self host docs](/self-host) guide.
- Drop into our [GitHub](https://github.com/theopenco/llmgateway) for help or feature requests.

Happy building! âœ¨
