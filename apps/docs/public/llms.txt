# Overview
URL: /

Introduction to LLM Gateway, an open-source API gateway for LLMs.

***

title: Overview
description: Introduction to LLM Gateway, an open-source API gateway for LLMs.
------------------------------------------------------------------------------

# LLM Gateway

LLM Gateway is an open-source API gateway for Large Language Models (LLMs). It acts as a middleware between your applications and various LLM providers, allowing you to:

* Route requests to multiple LLM providers (OpenAI, Anthropic, Google Vertex AI, and others)
* Manage API keys for different providers in one place
* Track token usage and costs across all your LLM interactions
* Analyze performance metrics to optimize your LLM usage

## Analyzing Your LLM Requests

LLM Gateway provides detailed insights into your LLM usage:

* **Usage Metrics**: Track the number of requests, tokens used, and response times
* **Cost Analysis**: Monitor spending across different models and providers
* **Performance Tracking**: Identify patterns and optimize your prompts based on actual usage data
* **Breakdown by Model**: Compare different models' performance and cost-effectiveness

All this data is automatically collected and presented in an intuitive dashboard, helping you make informed decisions about your LLM strategy.

## Getting Started

Using LLM Gateway is simple. Just swap out your current LLM provider URL with the LLM Gateway API endpoint:

```bash
curl -X POST https://api.llmgateway.io/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $LLM_GATEWAY_API_KEY" \
  -d '{
  "model": "gpt-4o",
  "messages": [
    {"role": "user", "content": "Hello, how are you?"}
  ]
}'
```

LLM Gateway maintains compatibility with the OpenAI API format, making migration seamless.

## Hosted vs. Self-Hosted

You can use LLM Gateway in two ways:

* **Hosted Version**: For immediate use without setup, visit [llmgateway.io](https://llmgateway.io) to create an account and get an API key.
* **Self-Hosted**: Deploy LLM Gateway on your own infrastructure for complete control over your data and configuration.

The self-hosted version offers additional customization options and ensures your LLM traffic never leaves your infrastructure if desired.


# Self Host LLMGateway
URL: /self-host

Simple guide to self-hosting LLMGateway using Docker Compose.

***

title: Self Host LLMGateway
description: Simple guide to self-hosting LLMGateway using Docker Compose.
--------------------------------------------------------------------------

# Self Host LLMGateway

LLMGateway is a self-hostable platform that provides a unified API gateway for multiple LLM providers. This guide will help you get started quickly using Docker Compose.

## What You'll Get

* **Gateway Service**: Routes LLM API requests to different providers
* **Web Interface**: Manage projects and API keys
* **Database**: Stores users, projects, and request logs
* **Redis**: Handles caching

## Prerequisites

* Latest Docker (which will include Docker Compose)
* API keys for the LLM providers you want to use (OpenAI, Anthropic, etc.)

## Quick Start

1. **Clone the repository**:

   ```bash
   git clone https://github.com/theopenco/llmgateway.git
   cd llmgateway
   ```

2. **Set up environment**:

   ```bash
   cp .env.example .env
   ```

3. **Configure your API keys** (see [Configuration](#configuration) section below)

4. **Start the services**:

   ```bash
   docker compose -f docker-compose.prod.yml up -d
   ```

5. **Initialize the database**:

   ```bash
   # Wait for services to start, then run:
   export DATABASE_URL=postgres://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-change_this_secure_password}@localhost:5432/${POSTGRES_DB:-llmgateway}
   pnpm migrate
   ```

6. **Access your LLMGateway**:
   * Web Interface: [http://localhost:3002](http://localhost:3002)
   * API Endpoint: [http://localhost:4002](http://localhost:4002)
   * Gateway: [http://localhost:4001](http://localhost:4001)

## Configuration

Edit the `.env` file to configure your LLMGateway instance:

### Required Settings

```bash
# Database (change the password!)
POSTGRES_PASSWORD=your_secure_password_here
DATABASE_URL=postgres://postgres:your_secure_password_here@postgres:5432/llmgateway

# Redis
REDIS_PASSWORD=your_redis_password_here

# Basic URLs
UI_URL=http://localhost:3002
API_URL=http://localhost:4002
ORIGIN_URL=http://localhost:3002
```

### LLM Provider API Keys

Add API keys for the providers you want to use:

```bash
# OpenAI
OPENAI_API_KEY=sk-...

# Anthropic
ANTHROPIC_API_KEY=sk-ant-...

# Google AI Studio
GOOGLE_AI_STUDIO_API_KEY=your_google_ai_key

# Add other providers as needed
```

That's it! The other settings in `.env.example` have sensible defaults.

## Managing Your Instance

### Basic Commands

```bash
# View logs
docker compose -f docker-compose.prod.yml logs -f

# Restart services
docker compose -f docker-compose.prod.yml restart

# Stop services
docker compose -f docker-compose.prod.yml down

# Update to latest version
docker compose -f docker-compose.prod.yml pull
docker compose -f docker-compose.prod.yml up -d
```

### Database Management

```bash
# Run database migrations (after updates)
export DATABASE_URL=postgres://postgres:your_password@localhost:5432/llmgateway
pnpm migrate
```

## Next Steps

Once your LLMGateway is running:

1. **Open the web interface** at [http://localhost:3002](http://localhost:3002)
2. **Create your first organization** and project
3. **Generate API keys** for your applications
4. **Test the gateway** by making API calls to [http://localhost:4001](http://localhost:4001)

For more information, see the [API Documentation](/docs/api) and [User Guide](/docs/guide).


# Models
URL: /v1/models

List all available models

***

title: Models
description: List all available models
full: true
\_openapi:
method: GET
route: /v1/models
toc: \[]
structuredData:
headings: \[]
contents:

* content: List all available models

***

{/* This file was generated by Fumadocs. Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"../gateway/openapi.json"} operations={[{"path":"/v1/models","method":"get"}]} webhooks={[]} hasHead={false} />


# Chat Completions
URL: /v1/chat/completions

Create a completion for the chat conversation

***

title: Chat Completions
description: Create a completion for the chat conversation
full: true
\_openapi:
method: POST
route: /v1/chat/completions
toc: \[]
structuredData:
headings: \[]
contents:

* content: Create a completion for the chat conversation

***

{/* This file was generated by Fumadocs. Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"../gateway/openapi.json"} operations={[{"path":"/v1/chat/completions","method":"post"}]} webhooks={[]} hasHead={false} />